---
title: 'Level 2 Data Cleaning: Clean the midwest Dataset'
---

# Objective

The objective of this assignment is to practice cleaning and transforming a messy dataset using tidyverse functions. You will use skills like renaming and reordering columns, sorting rows, changing data types, mutating data, and using the stringr and forcats packages.

This is the Level 2 Data Cleaning assignment. You may additionally or alternatively complete the [Level 1 Data Cleaning assignment](https://github.com/nrdowling/d2mr-assessment/tree/main/01_data-cleaning/01_cleaning-level-1), in which you will work with a simple dataset and focus on basic data cleaning tasks. The Level 1 assignment has more direct instruction and is recommended for those who are new to data cleaning.

In this Level 2 Cleaning assignment, you will work with a more complex dataset and perform additional cleaning tasks with less direct instruction. The Level 2 assignment has more opportunities to demonstrating meeting course standards than this Level 1 assignment and is recommended for those who are already comfortable with the tasks in this assignment.

# Instructions

1.  If you have not already done so, pull the latest changes from the `d2mr-assessment` repository to ensure you have the most up-to-date version of the assignment files. Confirm you are working in your fork of the repository.
2.  Open `cleaning-level-2.qmd` in RStudio and follow the instructions in the Setup section below to load and inspect the (original) `midwest` dataset.
    -   **Important:** Unlike Level 1, you will not be provided with a goal dataset to match. Instead, you will evaluate what cleaning tasks are necessary or useful *in principle*. You can reference the original `midwest` dataset, but ultimately you will need to decide what the "clean" version of the dataset should look like.
3.  Follow the guideline to identify and perform cleaning tasks on the `messy-midwest.csv` dataset.
4.  At some points in this document you may come across questions or non-coding exercises. Answer these questions in the text of this .qmd document, immediately below the question.
5.  *Optional:* Continue to follow the instructions in the assignment script to clean the dataset above and beyond matching the original.

# Setup

Run these chunks as written. Do not make changes to code except where noted if necessary.

## Loading libraries and set seed

```{r}
#| label: setup
library(tidyverse)
set.seed(1234)
```

## Read in and inspect messy data

Read in and inspect the messy dataset `messy-midwest.csv`.

```{r}

#| label: read-messy-data

### LEAVE THIS CHUNK AS-IS ###

# You *might* need to edit the filepath, but don't change anything else!

# Read in messy-midwest.csv
messy.midwest <- read_csv(
  ########################################
  "messy-midwest.csv", ## <-- THIS IS THE ONLY THING IN THIS CHUNK YOU CAN CHANGE IF NECESSARY
  ########################################
  trim_ws = FALSE, name_repair = "minimal", col_types = cols(.default = col_character()))

# Inspect the structure and contents of the messy midwest dataset with head(), glimpse(), str(), and/or View()
head(messy.midwest)
glimpse(messy.midwest)
str(messy.midwest)
View(messy.midwest)

```

## Inspect the original midwest dataset

```{r}
#| label: inspect-original-data

### LEAVE THIS CHUNK AS-IS ###

# Load the original midwest dataset
data(midwest)

# View the documentation for the midwest dataset
?midwest

# Inspect the structure and contents original midwest dataset with head(), glimpse(), str(), and/or View()
head(midwest)
glimpse(midwest)
str(midwest)
View(midwest)

```

QUESTIONS:

Q1. What are the differences between the messy dataset and the original midwest dataset?

<!-- answer below -->

There are a number of differences:

1.  The variable in the messy dataset are not properly named and their order is not neat. Specifically, the changes need to be made are:\
    a)'0' needs to be replaced by 'o'

<!-- -->

b)  all lower case except "PID"
c)  the variables starting with 'Popupation xxx' needs to be changed to 'popxxx'
d)  Each broader category's variables needs to stay together (i.e., population, ethinicity percentage, educational level percentage, poverty percentage)

<!-- -->

2.  PID in the orignial dataset is in order, but in the messy dataset it is in random order.
3.  In 'county',all upper case. Also, '1'needs to be replaced by 'l', '0' to be replaced by 'o' and '3' to be replaced by 'e'.
4.  In 'state',only keep the first two letter of the state's name, also it needs to be in upper case.
5.  Some values in 'Percentage Asian' and 'perother' are not avaialab (i.e., noted as 'NA'), however in the original dataset they are available.
6.  27 columns in the original dataset, but there are 28 in the messy one.

Q2. What are the biggest issues you need to address in cleaning?

<!-- answer below -->

1.  Why are some data not available in the messy dataset but they become avaiable in the original one?
2.  Why are there 28 columns in the original dataset, but only 27 in the messy dataset?
3.  Matching variable names can take a lot of work.

Q3. Are there any differences between the messy dataset and the original dataset that you do not need or want to address in cleaning? If so, why (not)?

<!-- answer below -->

1.  In the original dataset, there is a variable called "*per*college"---- I would rather change it to per*c*college, just to keep it consistent with other percentage variable names (i.e., 'perc' as an abbrevation for 'percentage' instead of 'per' )
2.  I probably would not change the 'NA's in the messy dataset to match the original dataset. The only way to do this that I can think of is to replace the 'NA's with the values from the original dataset, but in reality I would not just replace a value with another one. Instead, I would find out why there are dismatches first.
3.  Regarding the one extra column in the messy dataset, I would not delete it. In stead, I would move it to the very end so that it does not interfere with the all.equal() function.

Q4. Are there additional cleaning tasks you would like to perform beyond matching the original dataset? If so, what are they and why do you think they are important?

<!-- answer below -->

If this is my dataset, the first thing I will do is to screen out any data that is inapproapriate. For example, are the percentages less than 100? Other things that I would like to do is to keep 2 decimal places for all the numbers---- it is just easlier to observe whether there is anything wrong this way. I would also change percentages to 0.something instead of a number out of 100. This way when I need to perform some culculations I do not need to divide the numbers by 100.

# Cleaning

You may approach cleaning the dataset however you like based on how you identified problems above and how you think they should be prioritized.

If you're not sure where to start, you can organize your cleaning into the following categories. **You do not need to follow this structure.** Feel free to delete these sections, rearrange them, or add new ones as needed. (FYI: When I cleaned this myself I loosely followed this structure, but there were some parts of my approach that could not have worked in this order.)

You can additionally/alternatively construct your cleaning in a single pipeline in the last chunk.

## All cleaning in a single pipeline

```{r}

#| label: one-pipeline

clean.midwest <- messy.midwest %>%
  rename_with(~str_to_lower(str_replace_all(.,c("0"="o","Population "="pop","Percentage "="perc","Over 18"="adult","Name"=""," "=""))),-PID) 

#In variable namens, replace all the '0's with 'o' , 'Population ' with 'pop' ,'Percentage ' with 'perc', also delete 'Name'(from county name) and spaces. Change everything except for PID to lowercase.

all.equal(colnames(clean.midwest),colnames(midwest)) 
# So far, the message from all.equal is :"Lengths (27, 28) differ (string compare on first 27)","18 string mismatches" , so furhter work is needed

#At this stage I will check what the matching variable names are:

common_col<-intersect(colnames(clean.midwest),colnames(midwest))
print(common_col)

#Alternatively, I can use a more comlicated way to check the non-internecting column names:

non_common_col <- union(setdiff(colnames(clean.midwest),colnames(midwest)),setdiff(colnames(midwest),colnames(clean.midwest)))
print(non_common_col)

#Turns out that I have 22 common column names. I just need to deal with the rest 5(+1 that is an extra column). There are two pathways that I can take: 1. I can arrange the already-matching variables in order first, then deal with the rest. 2. I can manully change what does not match than match their orders. Since there are only 5 that do not match, I will just manully change the variable names.

clean.midwest <- clean.midwest %>%
  rename(area =`area(sqmiles)`,poptotal = totalpopulation, popadults = popadult, percollege = perccollege)

print(setdiff(colnames(clean.midwest),colnames(midwest)))
#Returns "poppersqmile", meaning clean.midwest has "poppersqmile" but midwest does not have this variable.
print(setdiff(colnames(midwest),colnames(clean.midwest)))
#Returns "percwhite" "percblack", meaning midwest has these variables but clean.midwest do not.

#Now I finally understand why midwest has 28 columns and the messy dataset has only 27. It's not that midwest has one extra dataset but two ---- and I will need to calculate these two columns and add them to the clean.dataset. Also "poppersqmile" is a column that does not exist in the original dataset. I will just keep it at the very end. 

#Now I am going to reorder clean.midwest's variable names based on the original dataset. 
#First I will creat a new dataset including a
properly_named_common_cols <-intersect(colnames(midwest),colnames(clean.midwest)) #make sure 'midwest' comes first so that the product vector follows the order of 'midwest'.

print(properly_named_common_cols)
clean.midwest<-clean.midwest %>%
  select(all_of(properly_named_common_cols),everything())
head(clean.midwest)

#Now let's check all.equal

all.equal(colnames(midwest),colnames(clean.midwest))

#Returns "Lengths (28, 27) differ (string compare on first 27)", "16 string mismatches" 

#I will put two dataset's variable names together and visualize them to find out what doesn't match

library(tibble)

header_tables<-(rbind(colnames(midwest),colnames(clean.midwest)))
print(header_tables)

#oK looks like I just need to calculate percentage white and percentage black and place them after "popother". But before doing that I need to change columns 1 & 4:25 to numeric.

clean.midwest <- clean.midwest %>%
  mutate(across(c(1,4:25,27),~as.numeric(.)),percwhite = popwhite/poptotal*100, percblack = popblack/poptotal*100) %>% 
  relocate(percwhite, .after = popother) %>%
  relocate(percblack, .after = percwhite)

header_tables<-(rbind(colnames(midwest),colnames(clean.midwest)))
print(header_tables)

all.equal(colnames(midwest), colnames(clean.midwest))
#Now all the variable names match (except that clean.midwest has one extra column at the very end which I want to keep )

#The next step is to:
# 1. Arrange PID in ascending order
# 2. In 'county',all values need to be in upper case. Also, '1'needs to be replaced by 'l', '0' to be replaced by 'o' and '3' to be replaced by 'e'. Also trim all the spaces.
# 3. In 'state',only keep the first two letter of the state's name, also it needs to be in upper case.

clean.midwest <- clean.midwest %>%
  arrange(PID) %>%
  mutate(
    county = county %>%
      str_to_upper() %>%
      str_replace_all(c("1" = "L", "0" = "O", "3" = "E"))%>%
      str_trim(),
    state = state %>%
      str_sub(1,2)%>%
      str_to_upper()
    )

all.equal(clean.midwest,midwest)

# still 1 dismatch in county

dismatch_clean <- setdiff(clean.midwest$county, midwest$county)
dismatch_midwest <- setdiff(midwest$county, clean.midwest$county)
print(dismatch_clean) 
print(dismatch_midwest)  
# So there is a "Winnebago" in midwest but not in clean.midwest.

which(midwest$county == "Winnebago")
# Turns out it's in role 101.
#Since in the original dataset 'midwest' county[101] is in lower case and we actually want everything to be in upper case, I'm actually just going to leave it as it is. 

#Lastly, I will deal with the 'NA' values in percasian and percother by recultulating the values. 

clean.midwest <- clean.midwest %>%
  mutate(percasian = popasian/poptotal*100, 
         percother = popother/poptotal*100
         )

all.equal(clean.midwest,midwest)

```

# Reflection

QUESTIONS:

1.  Is your dataset identical to `midwest`? If not, what is different? (Remember the functions `all.equal()` and `diff_data()` can help here.)

<!-- answer below -->

No. There are two differences. First, midwest$county[101] is in lower case (i.e., Winnebago), which is inconsistent from the rest of the data, so in clean.midwest$county\[101\] I just keep this value in upper case. Second, there is an extra column (population per square mile) in clean.midwest, which I figure might be useful, so I keep it at the very end so that it would not interfere with the comparison of the rest columns.

2.  Did you make any choices to clean the data that resulted in the dataset not matching the original? Why did you make those choices?

<!-- answer below -->

Actually I didn't do anything that resulted int the dataset not matching the original, except for clean.midwest\$county\[101\] which I kept at upper case to mathch with the overall format.

3.  Were there any cleaning steps -- whether necessary to recreate the original df or just because you wanted to do them -- that you weren't able to implement? If so, what were they and what more would you need to do/know to implement them?

<!-- answer below -->

# Unguided cleaning and transformation

*Optional:* If you have the time and interest, continue transforming this dataset as you please. Create new columns based on the existing ones, reformat strings, try your hand at a regex replacement, summarize by groups (factor levels), visualize a simple relationship, or anything else you can think of. To get you started, consider things like:

1.  **Exploratory Data Analysis:** Use the cleaned dataset to explore relationships between variables, create visualizations, and generate insights.
```{r}
# Create a summary for the dataset 
summary(clean.midwest$poptotal) #numeric variable
clean.midwest$county[which(clean.midwest$poptotal == 1701)]
clean.midwest$county[which(clean.midwest$poptotal == 5105067)]

# The total polulation in midwest counties ranged from 1701 people in Keweenaw to 5105067 people in Cook, with the average being 96130 and the median being 35324.

# Let's then create a population total for each state and order them in a descending manner.
state.population <- clean.midwest %>%
  group_by(state) %>%
  summarise(total_population = sum(poptotal))%>%
  arrange(desc(total_population))
print(state.population)

#We can also create a summary for the frequncy of category, grouped by states.

state.category <- clean.midwest %>%
  group_by(state, category) %>%
  summarise(count = n(), .groups = "drop") %>%
  arrange(state, desc(count))
  
print(state.category)

# We can see that all midwest states have the most AAR, except for Ohio, which has the mist AAU. Since the annotation for 'category' in the menu is 'Miscellaneous', we need to figure out what it means before further interpreting the result. 
```

2.  **Data Transformation:** Create new variables, aggregate data, or reshape the dataset to prepare it for analysis.
3.  **Split, Merge, and Reshape:** Split the dataset into multiple datasets or merge it with other datasets using `join` functions to create a new dataset. Use `pivot_longer()` and `pivot_wider()` to reshape the data.
4.  **Informativity:** Consider the midwest data and its documentation. Clean/transform the dataframe into a format that is more informative, transparent, or easier to work with. For example, improve column naming conventions, create new (useful) variables, reduce redundancy, or eliminate variables that are not useful or documented.

# Submission & Assessment

To submit:

1.  Add an `assessment.md` file to this mini-project's directory:
    1.  Check off all objectives you believe you have demonstrated
    2.  Indicate which unique objectives you are meeting for the first time (if any)
    3.  Complete any relevant open-ended items
2.  Push your changes to your centralized assignment repository on GitHub.
3.  Confirm that Dr. Dowling and your section TA are added as collaborators to your repository.
4.  Submit your work in your next open mini-project assignment by including the following information in the text box:
    1.  The title of the assignment: "Level 2 Data Cleaning: Clean the midwest Dataset"
    2.  A link to the **directory** for this assignment in your centralized assignment repo
